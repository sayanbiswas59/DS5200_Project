{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying_Abnormal_Normal_Study_wise_Densenet169.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTH6gRddjQJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oes1V0o-jtTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/MURA-v1.1.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp/SML\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTNUaWDWjxeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oICYNPVPj0lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/tmp/SML/MURA-v1.1/train_image_paths.csv\",\n",
        "                 names=[\"x_col\"])\n",
        "\n",
        "df['Region'] = df[\"x_col\"].map(lambda x: x.split('/')[-4])\n",
        "df[\"Region\"] = df[\"Region\"].map(lambda x: x.split('XR_')[-1])\n",
        "df[\"y_col\"] = df[\"x_col\"].map(lambda x: x.split('/')[-2])\n",
        "df[\"y_col\"] = df[\"y_col\"].map(lambda x: x.split('_')[-1])\n",
        "df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_SKSvMcj6yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Change region for each study type, if the the train and test are for each study type, else do not run this code\n",
        "df = df[df[\"Region\"]==\"FOREARM\"]\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWnx4Lzpj57G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"/tmp/SML/MURA-v1.1/valid_image_paths.csv\",\n",
        "                 names=[\"x_col\"])\n",
        "df_test['Region'] = df_test[\"x_col\"].map(lambda x: x.split('/')[-4])\n",
        "df_test[\"Region\"] = df_test[\"Region\"].map(lambda x: x.split('XR_')[-1])\n",
        "df_test[\"y_col\"] = df_test[\"x_col\"].map(lambda x: x.split('/')[-2])\n",
        "df_test[\"y_col\"] = df_test[\"y_col\"].map(lambda x: x.split('_')[-1])\n",
        "df_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieIArl83kURP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Change region for each study type, if the the train and test are for each study type, else do not run this code\n",
        "df_test = df_test[df_test[\"Region\"]==\"FOREARM\"]\n",
        "df_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TngAav9kWzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df[\"x_col\"]\n",
        "y = df[\"y_col\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGCpSzXNkaq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_path, X_test_path, y_train, y_test = train_test_split(X, y, test_size = 0.20,stratify=y, random_state = 57)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCltI6UPkdJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=pd.DataFrame()\n",
        "df_valid = pd.DataFrame()\n",
        "df_train[\"x_col\"]=X_train_path\n",
        "df_train[\"y_col\"]=y_train\n",
        "df_valid[\"x_col\"]=X_test_path\n",
        "df_valid[\"y_col\"]=y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0EgaG2GkiEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.densenet import DenseNet169, preprocess_input\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "# DATASET_PATH  = '/tmp/SML/MURA-v1.1/train/WRIST'\n",
        "IMAGE_SIZE    = (224, 224)\n",
        "NUM_CLASSES   = 1\n",
        "BATCH_SIZE    = 16  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
        "FREEZE_LAYERS = 2  # freeze the first this many layers for training\n",
        "NUM_EPOCHS    = 15\n",
        "WEIGHTS_FINAL = 'model-densenet-abnormal.h5'\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   channel_shift_range=10,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "train_batches = train_datagen.flow_from_dataframe(\n",
        "        dataframe=df_train,\n",
        "        directory='/tmp/SML',\n",
        "        x_col=\"x_col\",\n",
        "        y_col=\"y_col\",\n",
        "        interpolation='bicubic',\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=True,\n",
        "        seed =57)\n",
        "\n",
        "\n",
        "# train_batches = train_datagen.flow_from_directory(DATASET_PATH + '/train',\n",
        "#                                                   target_size=IMAGE_SIZE,\n",
        "#                                                   interpolation='bicubic',\n",
        "#                                                   class_mode='binary',\n",
        "#                                                   shuffle=True,\n",
        "#                                                   batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "valid_batches = valid_datagen.flow_from_dataframe(\n",
        "        dataframe=df_valid,\n",
        "        directory='/tmp/SML',\n",
        "        x_col=\"x_col\",\n",
        "        y_col=\"y_col\",\n",
        "        target_size=IMAGE_SIZE,\n",
        "        interpolation='bicubic',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary',\n",
        "        shuffle=False,\n",
        "        seed = 57)\n",
        "\n",
        "# valid_batches = valid_datagen.flow_from_directory(DATASET_PATH + '/valid',\n",
        "#                                                   target_size=IMAGE_SIZE,\n",
        "#                                                   interpolation='bicubic',\n",
        "#                                                   class_mode='binary',\n",
        "#                                                   shuffle=False,\n",
        "#                                                   batch_size=BATCH_SIZE)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_batches=test_datagen.flow_from_dataframe(\n",
        "dataframe=df_test,\n",
        "directory=\"/tmp/SML\",\n",
        "x_col=\"x_col\",\n",
        "y_col=None,\n",
        "batch_size=1,\n",
        "seed=57,\n",
        "shuffle=False,\n",
        "class_mode=None,\n",
        "target_size=IMAGE_SIZE)\n",
        "\n",
        "# show class indices\n",
        "print('****************')\n",
        "for cls, idx in train_batches.class_indices.items():\n",
        "    print('Class #{} = {}'.format(idx, cls))\n",
        "print('****************')\n",
        "\n",
        "# build our classifier model based on pre-trained ResNet50:\n",
        "# 1. we don't include the top (fully connected) layers of ResNet50\n",
        "# 2. we add a DropOut layer followed by a Dense (fully connected)\n",
        "#    layer which generates softmax class score for each class\n",
        "# 3. we compile the final model using an Adam optimizer, with a\n",
        "#    low learning rate (since we are 'fine-tuning')\n",
        "net = DenseNet169(include_top=False, weights='imagenet', input_tensor=None,\n",
        "               input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
        "x = net.output\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(1, activation='sigmoid', name='sigmoid')(x)\n",
        "net_final = Model(inputs=net.input, outputs=output_layer)\n",
        "for layer in net_final.layers[:FREEZE_LAYERS]:\n",
        "    layer.trainable = False\n",
        "for layer in net_final.layers[FREEZE_LAYERS:]:\n",
        "    layer.trainable = True\n",
        "net_final.compile(optimizer=Adam(lr=1e-5),\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# print(net_final.summary())\n",
        "\n",
        "# train the model\n",
        "history = net_final.fit_generator(train_batches,\n",
        "                        steps_per_epoch = train_batches.samples // BATCH_SIZE,\n",
        "                        validation_data = valid_batches,\n",
        "                        validation_steps = valid_batches.samples // BATCH_SIZE,\n",
        "                        epochs = NUM_EPOCHS)\n",
        "\n",
        "# save trained weights\n",
        "net_final.save(WEIGHTS_FINAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f40SlPPBlDcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batches.reset()\n",
        "pred=net_final.predict_generator(test_batches,\n",
        "steps=test_batches.samples//1,\n",
        "verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxZepu0lD82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "for i in pred:\n",
        "    if i >=0.5:\n",
        "        predictions.append('1')\n",
        "    else:\n",
        "        predictions.append('0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRPHju8sljQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_actual = df_test[\"y_col\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmAXtt9xlmQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_actual = y_actual.replace(\"negative\",0)\n",
        "y_actual = y_actual.replace(\"positive\",1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-DVH_17loMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=np.array(predictions).astype(dtype=\"int64\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlFV4mr_ltuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy on testing set:\",metrics.accuracy_score(y_actual, y_pred))\n",
        "print(\"Error on testing set:\",1-metrics.accuracy_score(y_actual, y_pred))\n",
        "print(\"Precision on testing set:\",metrics.precision_score(y_actual,y_pred))\n",
        "print(\"Recall on testing set:\",metrics.recall_score(y_actual, y_pred))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EExLAGyAlvxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}